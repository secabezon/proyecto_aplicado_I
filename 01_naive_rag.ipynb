{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ab4016b",
      "metadata": {
        "id": "9ab4016b"
      },
      "source": [
        "# Build a RAG agent with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9754c3",
      "metadata": {
        "id": "2a9754c3"
      },
      "source": [
        "## Overview\n",
        "\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
        "\n",
        "This class we will learn how to build a simple Q&A application over an unstructured text data source.\n",
        "\n",
        "We will demonstrate:\n",
        "- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
        "- A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54da7546",
      "metadata": {
        "id": "54da7546"
      },
      "source": [
        "## Concepts\n",
        "We will cover the following concepts:\n",
        "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\n",
        "- **Retrieval** and **generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "\n",
        "Once we’ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b862a1",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "a2b862a1"
      },
      "outputs": [],
      "source": [
        "$ uv add langchain langchain-text-splitters langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f6e5f1",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "d0f6e5f1"
      },
      "outputs": [],
      "source": [
        "$ uv add \"langchain[openai]\"\n",
        "# uv add \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c39d395",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "4c39d395",
        "outputId": "12efca08-5205-4fa0-80dc-6ccd897e0413"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30644e94",
      "metadata": {
        "id": "30644e94"
      },
      "source": [
        "# Components\n",
        "We will need to select three components from LangChain’s suite of integrations.\n",
        "1. Select a chat model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eeae792",
      "metadata": {
        "id": "0eeae792",
        "outputId": "2b6bd350-21f3-40ff-b5c1-a2b2deadde64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/felipecisternas/Desktop/uc/practicos-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"openai:gpt-4.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64313d3f",
      "metadata": {
        "id": "64313d3f"
      },
      "source": [
        "2. Select an embeddings model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c04802",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "a4c04802"
      },
      "outputs": [],
      "source": [
        "$ uv add langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa973f8",
      "metadata": {
        "id": "8fa973f8"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee66bca",
      "metadata": {
        "id": "4ee66bca"
      },
      "source": [
        "3. Select a vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb7ce79",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "cfb7ce79"
      },
      "outputs": [],
      "source": [
        "$ uv add langchain-qdrant \"langchain-core\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b573d9d",
      "metadata": {
        "id": "2b573d9d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5244de53",
      "metadata": {
        "id": "5244de53"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.models import Distance, VectorParams\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "vector_size = len(embeddings.embed_query(\"sample text\"))\n",
        "\n",
        "if not client.collection_exists(\"test\"):\n",
        "    client.create_collection(\n",
        "        collection_name=\"test\",\n",
        "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
        "    )\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"test\",\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dadfba",
      "metadata": {
        "id": "c1dadfba"
      },
      "source": [
        "# 1. Indexing\n",
        "\n",
        "Indexing commonly works as follows:\n",
        "1. **Load**: First we need to load our data. This is done with Document Loaders.\n",
        "2. **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won’t fit in a model’s finite context window.\n",
        "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4030fd89",
      "metadata": {
        "id": "4030fd89"
      },
      "source": [
        "## Loading documents\n",
        "We need to first load the blog post contents. We can use `DocumentLoaders` for this, which are objects that load in data from a source and return a list of `Document` objects.\n",
        "In this case we’ll use the `WebBaseLoader`, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text.\n",
        "\n",
        "We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
        "\n",
        "In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f275fb",
      "metadata": {
        "id": "b5f275fb",
        "outputId": "2a03c5c9-857b-4413-ee72-1a36b2e0d5c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total characters: 43047\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "assert len(docs) == 1\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1649fe",
      "metadata": {
        "id": "1e1649fe",
        "outputId": "2aeb3a65-4c09-45a8-df18-a39aff26222b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c86a4fe",
      "metadata": {
        "id": "2c86a4fe"
      },
      "source": [
        "## Splitting documents\n",
        "\n",
        "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
        "\n",
        "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n",
        "\n",
        "As in the semantic search tutorial, we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42beaa3c",
      "metadata": {
        "id": "42beaa3c",
        "outputId": "062fa5f2-cd99-46f7-d53e-69151194a807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split blog post into 63 sub-documents.\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87fbad4",
      "metadata": {
        "id": "d87fbad4"
      },
      "source": [
        "## Storing documents\n",
        "Now we need to index our 66 text chunks so that we can search over them at runtime.\n",
        "\n",
        "Our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\n",
        "\n",
        "We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e035bd",
      "metadata": {
        "id": "17e035bd",
        "outputId": "c0191c5e-6463-4135-aada-284ee8556e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['69e939f313224822abfbe25182c76049', '8078ae498a014d3da0a4d6654ef272eb', 'be3e00d1c57445c992e115ff8b2ceb03']\n"
          ]
        }
      ],
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "print(document_ids[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c05dc3",
      "metadata": {
        "id": "d7c05dc3"
      },
      "source": [
        "# 2. Retrieval and Generation\n",
        "RAG applications commonly work as follows:\n",
        "1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
        "2. **Generate**: A model produces an answer using a prompt that includes both the question with the retrieved data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a621f1bc",
      "metadata": {
        "id": "a621f1bc"
      },
      "source": [
        "Now let’s write the actual application logic.\n",
        "\n",
        "We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
        "\n",
        "We will demonstrate:\n",
        "- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
        "- A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bdf948",
      "metadata": {
        "id": "b4bdf948"
      },
      "source": [
        "## RAG agents\n",
        "One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84118645",
      "metadata": {
        "id": "84118645"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_context(query: str):\n",
        "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, retrieved_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3167d869",
      "metadata": {
        "id": "3167d869"
      },
      "source": [
        "Here we use the `@[tool decorator][tool]` to configure the tool to attach raw documents as artifacts to each `ToolMessage`.\n",
        "\n",
        "This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n",
        "\n",
        "Given our tool, we can construct the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb96adf",
      "metadata": {
        "id": "beb96adf"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "tools = [retrieve_context]\n",
        "# If desired, specify custom instructions\n",
        "prompt = (\n",
        "    \"You have access to a tool that retrieves context from a blog post. \"\n",
        "    \"Use the tool to help answer user queries.\"\n",
        ")\n",
        "agent = create_agent(model, tools, system_prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cd1ea9",
      "metadata": {
        "id": "80cd1ea9"
      },
      "source": [
        "Let’s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beead75b",
      "metadata": {
        "id": "beead75b"
      },
      "outputs": [],
      "source": [
        "query = (\n",
        "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
        "    \"Once you get the answer, look up common extensions of that method.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b40983",
      "metadata": {
        "id": "07b40983",
        "outputId": "c066f138-1bc6-4f8f-cc86-07e5ade731cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the standard method for Task Decomposition?\n",
            "\n",
            "Once you get the answer, look up common extensions of that method.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_lI0BfjBTxIX6ONTlLiHm47N3)\n",
            " Call ID: call_lI0BfjBTxIX6ONTlLiHm47N3\n",
            "  Args:\n",
            "    query: standard method for Task Decomposition\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578, '_id': 'de63b1e592ff49b88e43cf0a642d1bd3', '_collection_name': 'test'}\n",
            "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
            "Self-Reflection#\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, '_id': 'be3e00d1c57445c992e115ff8b2ceb03', '_collection_name': 'test'}\n",
            "Content: Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_C15jFLZearCVXv7X0Ehx76MN)\n",
            " Call ID: call_C15jFLZearCVXv7X0Ehx76MN\n",
            "  Args:\n",
            "    query: common extensions of chain of thought (CoT)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, '_id': 'be3e00d1c57445c992e115ff8b2ceb03', '_collection_name': 'test'}\n",
            "Content: Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518, '_id': '8787ad5726d2445eb5645c359cec1091', '_collection_name': 'test'}\n",
            "Content: Or\n",
            "@article{weng2023agent,\n",
            "  title   = \"LLM-powered Autonomous Agents\",\n",
            "  author  = \"Weng, Lilian\",\n",
            "  journal = \"lilianweng.github.io\",\n",
            "  year    = \"2023\",\n",
            "  month   = \"Jun\",\n",
            "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
            "}\n",
            "References#\n",
            "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
            "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
            "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
            "“ arXiv preprint arXiv:2302.02676 (2023).\n",
            "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
            "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
            "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
            "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The standard method for Task Decomposition is Chain of Thought (CoT) prompting. CoT instructs language models to \"think step by step,\" helping them break down complex tasks into smaller, simpler steps. This approach enhances performance and interpretability by transforming large tasks into manageable components.\n",
            "\n",
            "Common extensions of Chain of Thought include:\n",
            "\n",
            "- Tree of Thoughts (ToT): This method extends CoT by exploring multiple reasoning possibilities (multiple \"thoughts\") at each step, creating a tree structure. The search process in ToT can use algorithms like breadth-first search (BFS) or depth-first search (DFS), and each state can be evaluated by a prompt-based classifier or majority vote.\n",
            "\n",
            "Other notable approaches mentioned in this context include using external planners (like LLM+P) and combining reasoning with action as in the ReAct framework.\n"
          ]
        }
      ],
      "source": [
        "for event in agent.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ]\n",
        "    },\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b5bf53",
      "metadata": {
        "id": "83b5bf53"
      },
      "source": [
        "Note that the agent:\n",
        "1. Generates a query to search for a standard method for task decomposition.\n",
        "2. Receiving the answer, generates a second query to search for common extensions of it.\n",
        "3. Having received all necessary context, answers the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f507090",
      "metadata": {
        "id": "9f507090"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}